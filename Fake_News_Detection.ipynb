{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPourNnrhiqGlkzhzXpwm0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuheli31/Fake-News-Detector/blob/main/Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "from requests.utils import quote"
      ],
      "metadata": {
        "id": "msZS4f0WoM_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**G-News API (*Real News*)**"
      ],
      "metadata": {
        "id": "fOLut42OpXyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Real time news api (https://gnews.io/dashboard) -- 100 requests per day\n",
        "news_api_key = 'd2b27a1e3f818564545508b6c744d24a'\n",
        "#Endpoints are -> {search , top-headlines}\n",
        "endpoint = 'search'\n",
        "#Parameters are -> {The topic the user want to search}\n",
        "question = input('Enter your question: ')\n",
        "parameters = f'q={quote(question)}&lang=en&max=5'\n",
        "#Example (try this link to understand the url)-> https://gnews.io/api/v4/top-headlines?q=modi&lang=en&max=5&apikey=d2b27a1e3f818564545508b6c744d24a\n",
        "news_api_url = f'https://gnews.io/api/v4/{endpoint}?{parameters}&apikey={news_api_key}'\n",
        "\n",
        "response = requests.get(news_api_url)\n",
        "data = response.json()\n",
        "\n",
        "if 'articles' in data:\n",
        "    for article in data['articles']:\n",
        "        print(f\"Published Time: {article['publishedAt']}\")\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"Description: {article['description']}\")\n",
        "        print(f\"Image: {article['image']}\")\n",
        "        print(f\"Content: {article['content']}\")\n",
        "        print(f\"Source Name: {article['source']['name']}\")\n",
        "        print(f\"Source URL: {article['source']['url']}\")\n",
        "        print(f\"Source Country: {article['source']['country']}\")\n",
        "        print(f\"URL: {article['url']}\\n\")\n",
        "else:\n",
        "    print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd6poGBt6bJi",
        "outputId": "893f389a-4c87-4cf7-bc75-4062c3c4d214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: g20 meet\n",
            "Published Time: 2025-08-21T12:06:08Z\n",
            "Title: India-Russia To Strengthen Trade Ties To Offset Trump Tariffs, BIG Takeaways From Jaishankar-Lavrov Meet\n",
            "Description: EAM S. Jaishankar and Russian Foreign Minister Sergey Lavrov reaffirmed India–Russia’s deep-rooted ties in Moscow, pledging cooperation in labour, trade, and connectivity. The leaders announced new consulates, backed anti-terror efforts, and vowed closer engagement in BRICS, G20, and SCO.\n",
            "Image: https://img.republicworld.com/all_images/jaishankar-lavrov-meet-1755781005544-16_9.webp\n",
            "Content: External Affairs Minister S. Jaishankar and his Russian counterpart Sergey Lavrov on Thursday underlined that India–Russia ties remain anchored in “trust” and “shared interests”, while opening the door to new avenues of cooperation. The leaders addre... [1929 chars]\n",
            "Source Name: Republic World\n",
            "Source URL: https://www.republicworld.com\n",
            "Source Country: in\n",
            "URL: https://www.republicworld.com/business/india-russia-to-strengthen-trade-ties-to-offset-trump-tariffs-big-takeaways-from-jaishankar-lavrov-meet\n",
            "\n",
            "Published Time: 2025-08-20T06:38:13Z\n",
            "Title: 2017 Row When Trump Confiscated His Translator's Notes After Putin Meet | Firstpost America | N18G\n",
            "Description: In 2017, Donald Trump met Russian President Vladimir Putin privately during the G20 summit. Reports later revealed that Trump took the interpreter’s notes and told them not to share details, not even with his own administration. This sparked outrage in Washington, as past presidents usually kept detailed records of such meetings. From 2017 to 2019, Trump held at least five private talks with Putin without full official records. In 2019, two watchdog groups sued the State Department, accusing then–Secretary of State Mike Pompeo of violating federal records law for failing to preserve the Hamburg meeting notes. The notes were never recovered. With Trump and Putin set to meet in Alaska on August 15, the 2017 secrecy is back in focus.\n",
            "Image: https://media.nw18.com/media-assets/wbx6pq/08-2025/20-2025/lNJ0Q0il4j-1308-TRUMP-TRANSLATORVODjpg-8uCPtB4N9j.jpg?im=FitAndFill,width=1200,height=675\n",
            "Content: 2017 Row When Trump Confiscated His Translator's Notes After Putin Meet | Firstpost America | N18G\n",
            "In 2017, Donald Trump met Russian President Vladimir Putin privately during the G20 summit. Reports later revealed that Trump took the interpreter’s no... [598 chars]\n",
            "Source Name: Firstpost\n",
            "Source URL: https://www.firstpost.com\n",
            "Source Country: in\n",
            "URL: https://www.firstpost.com/web-show/firstpost-america/2017-row-when-trump-confiscated-his-translators-notes-after-putin-meet-firstpost-america-n18g-vd1369027/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Twitter API**"
      ],
      "metadata": {
        "id": "Y0X91AiGrgx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for X (Twitter) API interaction\n",
        "import tweepy\n",
        "\n",
        "# Hardcoded Bearer Token\n",
        "X_API_BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAKCJ4QEAAAAAcEQy2tIw03BHpMFUg3fg9sbpGu8%3Db2Q2D7tbJUI2A0fJjOOMz69iKILmCmXeBYwVNNoxOaty3y2Oj2\"\n",
        "\n",
        "#Searching in tweets\n",
        "question = input('Enter your search query for X (Twitter): ').strip()\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if not X_API_BEARER_TOKEN or not question:\n",
        "    print(\"ERROR: Both the Bearer Token and the search query are required.\")\n",
        "else:\n",
        "    try:\n",
        "        # Initialize Tweepy Client\n",
        "        client = tweepy.Client(X_API_BEARER_TOKEN)\n",
        "\n",
        "        # API Call with image support\n",
        "        response = client.search_recent_tweets(\n",
        "            query=f'{question} -is:retweet has:images lang:en',\n",
        "            max_results=10,\n",
        "            tweet_fields=['created_at', 'public_metrics', 'attachments'],\n",
        "            user_fields=['name', 'username', 'verified'],\n",
        "            media_fields=['url', 'preview_image_url', 'type'],\n",
        "            expansions=['author_id', 'attachments.media_keys']\n",
        "        )\n",
        "\n",
        "        if not response or not response.data:\n",
        "            print(\"\\nNo recent tweets found for that query.\")\n",
        "        else:\n",
        "            tweets = response.data\n",
        "            users = {user['id']: user for user in response.includes.get('users', [])} if response.includes else {}\n",
        "            media = {m['media_key']: m for m in response.includes.get('media', [])} if response.includes else {}\n",
        "\n",
        "            print(\"\\n--- Found Tweets ---\\n\")\n",
        "            for tweet in tweets:\n",
        "                author = users.get(tweet.author_id)\n",
        "                media_urls = []\n",
        "\n",
        "                # Extract media URLs\n",
        "                if hasattr(tweet, 'attachments') and 'media_keys' in tweet.attachments:\n",
        "                    for key in tweet.attachments['media_keys']:\n",
        "                        if key in media and media[key].type == 'photo':\n",
        "                            media_urls.append(media[key].url)\n",
        "\n",
        "                tweet_url = f\"https://x.com/{author.username}/status/{tweet.id}\" if author else None\n",
        "                author_url = f\"https://x.com/{author.username}\" if author else None\n",
        "\n",
        "                print(f\"Published Time: {tweet.created_at}\")\n",
        "                print(f\"Tweet Text: {tweet.text}\")\n",
        "                if author:\n",
        "                    print(f\"Author Name: {author.name} (@{author.username})\")\n",
        "                    print(f\"Author URL: {author_url}\")\n",
        "                print(f\"Tweet URL: {tweet_url}\")\n",
        "\n",
        "                if media_urls:\n",
        "                    print(\"Image URLs:\")\n",
        "                    for img in media_urls:\n",
        "                        print(f\"- {img}\")\n",
        "                else:\n",
        "                    print(\"No images found.\")\n",
        "\n",
        "                print(\"\\n\")\n",
        "\n",
        "    except tweepy.errors.Unauthorized:\n",
        "        print(\"\\n--- AUTHENTICATION ERROR ---\")\n",
        "        print(\"The Bearer Token you provided is invalid. Please check your credentials on the X Developer Portal.\")\n",
        "    except tweepy.errors.BadRequest:\n",
        "        print(\"\\n--- INVALID SEARCH QUERY ---\")\n",
        "        print(\"Your search query is invalid or too complex. Please check the format.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred.\")\n",
        "        print(f\"Error details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCAB5yObj148",
        "outputId": "06f63a02-1e04-44a1-bb58-c686c9876449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your search query for X (Twitter): moon\n",
            "\n",
            "An unexpected error occurred.\n",
            "Error details: 429 Too Many Requests\n",
            "Too Many Requests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reddit API**"
      ],
      "metadata": {
        "id": "9YCLcNrFrk_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG4IcqMhsL9r",
        "outputId": "fdf72a79-2d0b-4e03-a042-494dde5fb901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.12/dist-packages (from praw) (1.9.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.10.5)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for Reddit API interaction\n",
        "import praw\n",
        "import datetime # To format the timestamp from Reddit\n",
        "\n",
        "# --- API Configuration ---\n",
        "REDDIT_CLIENT_ID = 'utnSmiRRrv9ZDIlJ9LveMg'\n",
        "REDDIT_CLIENT_SECRET = 'jzP9XRolmRsu3CNiDZmKF0IDDGcXpA'\n",
        "REDDIT_USER_AGENT = 'python:moonsearcher:v1.0 (by /u/Dependent_Car_5879)'\n",
        "\n",
        "# --- Search Parameters ---\n",
        "try:\n",
        "    question = input('Enter your search query for Reddit: ').strip()\n",
        "    limit_str = input('How many results would you like? (e.g., 10): ').strip()\n",
        "    num_results = int(limit_str) if limit_str.isdigit() else 10\n",
        "except EOFError:\n",
        "    print(\"\\nNo input provided for search query. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET or not question:\n",
        "    print(\"ERROR: Client ID, Client Secret, and a search query are required.\")\n",
        "else:\n",
        "    try:\n",
        "        reddit = praw.Reddit(\n",
        "            client_id=REDDIT_CLIENT_ID,\n",
        "            client_secret=REDDIT_CLIENT_SECRET,\n",
        "            user_agent=REDDIT_USER_AGENT,\n",
        "        )\n",
        "\n",
        "        search_results = reddit.subreddit(\"all\").search(\n",
        "            query=question,\n",
        "            sort='new',\n",
        "            limit=num_results\n",
        "        )\n",
        "\n",
        "        submissions = list(search_results)\n",
        "\n",
        "        if not submissions:\n",
        "            print(\"\\nNo recent posts found for that query.\")\n",
        "        else:\n",
        "            print(f\"\\n--- Found {len(submissions)} Posts ---\\n\")\n",
        "            for submission in submissions:\n",
        "                author_name = submission.author.name if submission.author else \"[deleted]\"\n",
        "                post_url = f\"https://www.reddit.com{submission.permalink}\"\n",
        "                author_url = f\"https://www.reddit.com/user/{author_name}\" if author_name != \"[deleted]\" else \"N/A\"\n",
        "\n",
        "                published_time = datetime.datetime.fromtimestamp(submission.created_utc)\n",
        "\n",
        "                # --- Print basic info ---\n",
        "                print(f\"Published Time: {published_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                print(f\"Post Title: {submission.title}\")\n",
        "                print(f\"Subreddit: r/{submission.subreddit.display_name}\")\n",
        "                print(f\"Score: {submission.score} (Upvote Ratio: {submission.upvote_ratio * 100:.0f}%)\")\n",
        "                print(f\"Author: u/{author_name}\")\n",
        "                print(f\"Author URL: {author_url}\")\n",
        "                print(f\"Post URL: {post_url}\")\n",
        "\n",
        "                # --- Print full text if available ---\n",
        "                if submission.selftext:\n",
        "                    print(f\"Post Text: {submission.selftext.replace(chr(10), ' ')}\")\n",
        "\n",
        "                # --- Extract image if any ---\n",
        "                image_url = None\n",
        "                if submission.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                    image_url = submission.url\n",
        "                elif hasattr(submission, 'preview'):\n",
        "                    try:\n",
        "                        image_url = submission.preview['images'][0]['source']['url']\n",
        "                    except (KeyError, IndexError):\n",
        "                        pass\n",
        "\n",
        "                if image_url:\n",
        "                    print(f\"Image URL: {image_url}\")\n",
        "                else:\n",
        "                    print(\"Image: None\")\n",
        "\n",
        "                print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "    except praw.exceptions.PRAWException as e:\n",
        "        print(\"\\n--- API ERROR ---\")\n",
        "        print(\"An error occurred with the Reddit API. This could be due to invalid credentials or a connection problem.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred.\")\n",
        "        print(f\"Error details: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEzHFodMoLVj",
        "outputId": "446664c0-31c7-4983-8d2b-d6c25c6ecb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your search query for Reddit: roads india\n",
            "How many results would you like? (e.g., 10): 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Found 10 Posts ---\n",
            "\n",
            "Published Time: 2025-10-16 06:35:51\n",
            "Post Title: Misled by Triumph showroom about price hike — lost ₹20-25k before Diwali offers\n",
            "Subreddit: r/triumph4India\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/cipherthread\n",
            "Author URL: https://www.reddit.com/user/cipherthread\n",
            "Post URL: https://www.reddit.com/r/triumph4India/comments/1o7yzfm/misled_by_triumph_showroom_about_price_hike_lost/\n",
            "Post Text:  I went to the Triumph showroom on 10th September after hearing that prices would increase after 22nd September due to a GST revision. The showroom staff also confirmed this and advised me to book the bike before 22nd to avoid the price hike.  So I paid 100k on 14th September and they completed the RTO registration and insurance by 20th September later i found out that the prices actually didn't increase after 22nd September and in fact Triumph launched Diwali offers in October.  I had clearly told them that I wanted delivery on 18th October (Dhanteras), but they still pushed me to book early under the false impression of a price hike. Now I have effectively lost ₹20–25k because of their misleading information.  The RC and insurance are already issued, and I will be paying the remaining ₹1.8 lakh on 18th October at delivery.  Is there anything I can do at this point maybe raise a complaint with Triumph India or the dealership?   The total amount which I am paying the dealership is ₹280k excluding insurance which I purchased from acko ₹20k.   On road price - ₹301k almost  On road price after Diwali offers- ₹275k-₹280k.  \n",
            "Image URL: https://i.redd.it/kjxtehcf5fvf1.jpeg\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 05:44:12\n",
            "Post Title: How to Reach Bhutan from India: Complete Travel Guide\n",
            "Subreddit: r/u_Sad-Feed2722\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/Sad-Feed2722\n",
            "Author URL: https://www.reddit.com/user/Sad-Feed2722\n",
            "Post URL: https://www.reddit.com/r/u_Sad-Feed2722/comments/1o7y62d/how_to_reach_bhutan_from_india_complete_travel/\n",
            "Post Text: Dreaming of the mystical mountains, serene monasteries, and winding roads of Bhutan? If you’re wondering how to reach Bhutan from India, this detailed guide will help you plan the perfect journey — whether you’re flying in or riding across the Himalayas.  # Why Visit Bhutan?  Known as the Land of the Thunder Dragon, Bhutan offers unmatched beauty, spirituality, and adventure. The country’s peaceful vibe and scenic landscapes attract travelers, trekkers, and bikers from all over the world.  For riders, the ultimate adventure is a[ Bhutan Motorcycle Tour](https://thedreamridersgroup.com/International/Bhutan.php) — a breathtaking expedition through mountain passes, lush valleys, and culturally rich towns.\n",
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 05:29:59\n",
            "Post Title: Left the game 10 years ago at level 47, downloaded it again 4 days ago now I am at 79 level!\n",
            "Subreddit: r/8BallPool\n",
            "Score: 0 (Upvote Ratio: 50%)\n",
            "Author: u/Artistic_Culture_111\n",
            "Author URL: https://www.reddit.com/user/Artistic_Culture_111\n",
            "Post URL: https://www.reddit.com/r/8BallPool/comments/1o7xxne/left_the_game_10_years_ago_at_level_47_downloaded/\n",
            "Image URL: https://i.redd.it/jnp5o0iptevf1.jpeg\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 05:12:36\n",
            "Post Title: Hyundai Motor shares in focus as automaker bets Rs 45,000 crore on India push: Economictimes\n",
            "Subreddit: r/karobar\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/karobar-news-bot\n",
            "Author URL: https://www.reddit.com/user/karobar-news-bot\n",
            "Post URL: https://www.reddit.com/r/karobar/comments/1o7xn61/hyundai_motor_shares_in_focus_as_automaker_bets/\n",
            "Image URL: https://external-preview.redd.it/mr2Bxh-uku6NasKgCCxwlpjmmsOswr5GVSfu_8AgE08.jpeg?auto=webp&s=7e57418965bf8320d45c626323d33f21ce37aa4b\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 04:23:06\n",
            "Post Title: State to ready 300 -acre data centre park at Tavarekere in 18 months\n",
            "Subreddit: r/bangalore\n",
            "Score: 11 (Upvote Ratio: 100%)\n",
            "Author: u/222homelander\n",
            "Author URL: https://www.reddit.com/user/222homelander\n",
            "Post URL: https://www.reddit.com/r/bangalore/comments/1o7wrz1/state_to_ready_300_acre_data_centre_park_at/\n",
            "Image URL: https://i.redd.it/k4pvc60shevf1.jpeg\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 04:18:56\n",
            "Post Title: Selling Wishcare Hair Growth Serum and Derma co vitamin C moisturizer with freebies\n",
            "Subreddit: r/ThriftIndia\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/ValidIntent\n",
            "Author URL: https://www.reddit.com/user/ValidIntent\n",
            "Post URL: https://www.reddit.com/r/ThriftIndia/comments/1o7wpc7/selling_wishcare_hair_growth_serum_and_derma_co/\n",
            "Post Text: I can't arrange shipping. These are just my impulsive buys. Please help me out  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 04:14:42\n",
            "Post Title: Some people deserve to be poor\n",
            "Subreddit: r/pune\n",
            "Score: 10 (Upvote Ratio: 66%)\n",
            "Author: u/ayxsh_03\n",
            "Author URL: https://www.reddit.com/user/ayxsh_03\n",
            "Post URL: https://www.reddit.com/r/pune/comments/1o7wmjw/some_people_deserve_to_be_poor/\n",
            "Post Text: Sometimes I genuinely feel that some people deserve to be poor because of the way they behave. In India, a shocking number of people lack even the most basic civic sense, and the rest of us end up paying the price for their ignorance and arrogance. Just yesterday, I faced three separate incidents in traffic that made me lose faith in people's sense of responsibility. Even after giving the correct left indicator, a scooty guy behind me kept honking aggressively, glaring at me, and even threatening to beat me up. It's unbelievable how people can be so impatient and uncivilized. Later, at a green signal that clearly allowed us to go, an auto rickshaw driver stopped right in the middle of the road just to buy a cigarette. When I honked at him for nearly a minute, instead of realizing his mistake, he started abusing me! Because of his stupidity, traffic got blocked for everyone. Honestly, when people behave like this completely disregarding rules, logic, and respect for others they create their own misery. Their lack of sense, discipline, and basic decency keeps them trapped in the same cycle of struggle and poverty. And in a way, that's exactly what they deserve.\n",
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 04:05:16\n",
            "Post Title: Client Story: Why I Chose Dholera Over Other Gujarat Cities\n",
            "Subreddit: r/u_Left-Page-9628\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/Left-Page-9628\n",
            "Author URL: https://www.reddit.com/user/Left-Page-9628\n",
            "Post URL: https://www.reddit.com/r/u_Left-Page-9628/comments/1o7wghb/client_story_why_i_chose_dholera_over_other/\n",
            "Post Text: I wanted to share something real from my own journey that might help anyone thinking about investing in Gujarat’s upcoming real estate hubs.  A couple of years ago, I was actively exploring property investment in Gujarat — mainly Ahmedabad outskirts, Gift City, and Vadodara. Every place looked promising on paper, but most were already saturated or overvalued. Then, a friend mentioned **Dholera Smart City**, and honestly, I was skeptical at first. Everyone talks about it like “the next Dubai of India,” but I wanted facts, not hype.  That’s when I came across **Dholera Smart City Solutions**, a local consultancy that focuses entirely on Dholera projects. What stood out to me was how transparent they were. Instead of pushing me to “book fast,” they took time to explain the government’s master plan, land use zones, RERA approvals, and even long-term connectivity updates like the expressway and international airport.  When I finally visited Dholera, it felt completely different from any other city project I’d seen — well-planned roads, proper infrastructure under development, and genuine government involvement. I realized this wasn’t just another private township; it was part of India’s official smart city mission.  Long story short, I ended up buying a residential plot through **Dholera Smart City Solutions**, and it’s been one of my best investment decisions so far. Prices have steadily appreciated, and with all the infrastructure work happening, I feel confident this is only the beginning.  I’m not here to sell anything — just to share my experience for those genuinely curious about the Dholera project. If you’re exploring Gujarat’s real estate and want to make an informed decision, I’d definitely recommend talking to a verified consultant before investing blindly. For me, **Dholera Smart City Solutions** made that difference.  Happy to answer any questions if someone’s researching the area or considering property in Gujarat.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 03:56:23\n",
            "Post Title: Fix your country\n",
            "Subreddit: r/IndianTeenagers\n",
            "Score: 1 (Upvote Ratio: 100%)\n",
            "Author: u/No-Match-6725\n",
            "Author URL: https://www.reddit.com/user/No-Match-6725\n",
            "Post URL: https://www.reddit.com/r/IndianTeenagers/comments/1o7wafr/fix_your_country/\n",
            "Post Text: I just wanna say maturing is realising how diverse and big india actually is , and leave the motherland is the retarded way of solving shit ,   I have heard of some garbage cafe where you get food in exchange of plastic waste and many other thing , and I have seen individuals and organisations working to clean or country by all means of fixing , maybe it's the minorities of people that runs   idk why I am Posting this here but it just feels right , be aware, save your trash and put it in the right, not on the side of the road . If you want high living standards make it yourself, it for everyone and throw trash and make streets and countryside and tourist places dirty and trash less attractive , start loving your country   loving a flower is easy but it's takes a lot to love a leave   It's not cringe , okay\n",
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Published Time: 2025-10-16 03:49:04\n",
            "Post Title: Tamilnadu Corrupt Society\n",
            "Subreddit: r/TamilNadu\n",
            "Score: 0 (Upvote Ratio: 44%)\n",
            "Author: u/Klutzy_Agency_1992\n",
            "Author URL: https://www.reddit.com/user/Klutzy_Agency_1992\n",
            "Post URL: https://www.reddit.com/r/TamilNadu/comments/1o7w5gu/tamilnadu_corrupt_society/\n",
            "Post Text: Why are most  tamil people in generally Indians so pathetic, stupid , corrupt..inknownthe question is so hard to digest and so direct, but yeah...I am a middle class, al my political leaders are corrupt, making money out of corruption, becoming multiple crorepati in years when common people put vote for 27 paise per day..that's 500rs for 5 years...is it because tamil people does not have maanam soodu sorana. Our State CM boast that Tamil Nadu is the most developed in India but our inside city and town roads are full.of.potholes , can't move a document in govt office without bribe, Dmk ruling party people are so rowdy nature harrassing people , people spit and throw garbage on roads . I have been to Sri Lanka were they r poorer than us but roads are very clean......Then we say we are Cholan Pandyan parambara...when we are a degenerated society...Common people FIR are not taken in police station if we don't have political background...        Our cities are unplanned catastrophic,alloting residential layout in pond areas ..no proper planning,all our big cities are flooded with traffic, potholes, people falling in open sewage holes & kids dying by uncontrolled stray dog bites, sewage mixing with fresh water when flooding happens.      Seeing all this happening around me I can't digest the society and have to go through anxiety everyday. I have travelled to many countries but nothing is pathetic as Indian civil society..even some African countries are better than us.     I am a father now and even think & wonder whether I need to keep my kids living in this Tamil indian society. I am not affiliated to any political party, just a common man...    In future all our ebbill and property tax are going to double triple  to pay the debt of corrupt Indian politicians who give tender to companies that pay 40% kick back to admk dmk pockets .     Should I send my kids out of Tamil Nadu/ India for good ...? Kindly advise or is there any hope for our people and society!\n",
            "Image: None\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "yzWJMI0h1Lst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgDRyrOE1Qi0",
        "outputId": "29c64052-7de3-4cd6-b31f-7c9b02556418"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/608.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 spacy scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k9Q2tEq1Y0J",
        "outputId": "ee2a9706-fb3d-4328-e3fc-b7b5be1f42de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivgadaDi1bca",
        "outputId": "00278a04-748d-4bbd-f010-0eed78a4f30e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import emoji\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Load English model for NLP (NER, sentence segmentation)\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # or \"en_core_web_trf\" for transformer-based embeddings\n",
        "\n",
        "# --- Helper functions ---\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text: remove HTML, emojis, normalize whitespace, lowercase.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Remove emojis\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Lowercase\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "def extract_urls(text):\n",
        "    \"\"\"Extract URLs from text\"\"\"\n",
        "    return re.findall(r'https?://\\S+', text)\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    \"\"\"Extract hashtags\"\"\"\n",
        "    return re.findall(r'#\\w+', text)\n",
        "\n",
        "def normalize_timestamp(ts):\n",
        "    \"\"\"Convert timestamp string to UTC datetime\"\"\"\n",
        "    try:\n",
        "        return datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_claims(text):\n",
        "    \"\"\"Extract potential claims from text using sentence segmentation + NER\"\"\"\n",
        "    doc = nlp(text)\n",
        "    claims = []\n",
        "    for sent in doc.sents:\n",
        "        # If sentence contains at least one named entity, consider as claim candidate\n",
        "        if len(sent.ents) > 0:\n",
        "            claims.append(sent.text.strip())\n",
        "    return claims\n",
        "\n",
        "def cluster_claims(claims, n_clusters=5):\n",
        "    \"\"\"Optional: cluster similar claims using TF-IDF + AgglomerativeClustering\"\"\"\n",
        "    if not claims:\n",
        "        return []\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(claims)\n",
        "    n_clusters = min(n_clusters, len(claims))\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(X.toarray())\n",
        "    clustered_claims = {}\n",
        "    for idx, label in enumerate(clustering.labels_):\n",
        "        clustered_claims.setdefault(label, []).append(claims[idx])\n",
        "    return clustered_claims\n",
        "\n",
        "# --- Example processing function ---\n",
        "\n",
        "def process_post(post_json):\n",
        "    \"\"\"\n",
        "    Input: raw JSON of a tweet or Reddit post\n",
        "    Output: normalized dict with cleaned text, claims, URLs, hashtags, user info\n",
        "    \"\"\"\n",
        "    # Text\n",
        "    text = post_json.get('text') or post_json.get('selftext') or \"\"\n",
        "    clean = clean_text(text)\n",
        "\n",
        "    # URLs & hashtags\n",
        "    urls = extract_urls(clean)\n",
        "    hashtags = extract_hashtags(clean)\n",
        "\n",
        "    # Timestamp\n",
        "    timestamp = normalize_timestamp(post_json.get('created_utc') or post_json.get('created_at'))\n",
        "\n",
        "    # User info (public only)\n",
        "    user_info = post_json.get('user') or post_json.get('author')\n",
        "    user_data = {}\n",
        "    if user_info:\n",
        "        user_data = {\n",
        "            'username': user_info.get('screen_name') or user_info.get('name') or user_info.get('author'),\n",
        "            'followers_count': user_info.get('followers_count', 0),\n",
        "            'following_count': user_info.get('friends_count', 0),\n",
        "            'account_age_days': (datetime.utcnow() - normalize_timestamp(user_info.get('created_at'))).days\n",
        "            if user_info.get('created_at') else None,\n",
        "            'verified': user_info.get('verified', False)\n",
        "        }\n",
        "\n",
        "    # Claims extraction\n",
        "    claims = extract_claims(clean)\n",
        "\n",
        "    return {\n",
        "        'clean_text': clean,\n",
        "        'urls': urls,\n",
        "        'hashtags': hashtags,\n",
        "        'timestamp': timestamp,\n",
        "        'user': user_data,\n",
        "        'claims': claims\n",
        "    }\n",
        "\n",
        "# --- Example usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example post JSON\n",
        "    sample_post = {\n",
        "        \"text\": \"India-Russia trade ties are strengthening! Read more: https://example.com/news #G20 #Trade\",\n",
        "        \"created_at\": \"2025-10-16T03:49:04Z\",\n",
        "        \"user\": {\n",
        "            \"screen_name\": \"Klutzy_Agency_1992\",\n",
        "            \"followers_count\": 1200,\n",
        "            \"friends_count\": 300,\n",
        "            \"created_at\": \"2018-06-01T12:00:00Z\",\n",
        "            \"verified\": False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    processed = process_post(sample_post)\n",
        "    print(json.dumps(processed, indent=4))\n",
        "\n",
        "    # Optional: cluster claims\n",
        "    clustered = cluster_claims(processed['claims'], n_clusters=3)\n",
        "    print(\"\\nClustered Claims:\")\n",
        "    print(json.dumps(clustered, indent=4))\n"
      ],
      "metadata": {
        "id": "yUlCwIawrz49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "04ec9072-fb71-4b9e-c972-fb413b5e22f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'emoji'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-848022716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDlL0c3G1BIE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}